---
title: "Modeling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(DAAG)
library(class)
library(ROCR)
library(gbm)
library(adabag)
```

```{r}

CVmodel_accuracy <- function(classifier, data, K, loss, featstrain, labeltrain){

  #still the error
  folds <- createFolds(data$cloud_label, k = K)
  fold_loss <- rep(0,K)
  
  for (i in 1:K){
    val_dat <- data[folds[[i]],]
    train_dat <- data[-folds[[i]],]
    
    #knn
    if (classifier=="knn"){
      datclassed <- knn(train_dat[,4:6], val_dat[,4:6],train_dat$cloud_label,k=10)
    }
    
    #logistic
    else if (classifier=="glm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- glm(formula, data=train_dat, family="binomial")
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    
    #lda
    else if(classifier=="lda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- lda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #qda
    else if(classifier=="qda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- qda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #boosting
    else if(classifier=="gbm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- gbm(formula, data = train_dat, distribution = "adaboost") 
      #getting higher accuracy with bernoulli instead of gaussian
      #getting higher accuracy with adaboost (exponential loss)
      model.pred <- predict(model, val_dat, n.trees = 100)
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }
    #This part is the problem of QDA
    fold_loss[i] <- loss(datclassed, val_dat$cloud_label) 
  }  
  return(fold_loss)
}

```

## 3. We now try to fit different classification models and assess the fitted models using different criterion. For the next three parts, we expect you to try logistic regression and at least three other methods. 

Since we're only using CV to choose models, we merge validation set into the training set 
```{r}
a1 <- timagestrain <- rbind(timagestrain, timagesval)
a2 <- imagestrain <- rbind(imagestrain, imagesval)
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Add cl column for models that want binary inputs
timagestrain <- mutate(timagestrain, cl=ifelse(cloud_label==1, 1, 0))
timagestest <- mutate(timagestest, cl=ifelse(cloud_label==1, 1, 0))
imagestrain <- mutate(imagestrain, cl=ifelse(cloud_label==1, 1, 0))
imagestest <- mutate(imagestest, cl=ifelse(cloud_label==1, 1, 0))

#dataframe setup
rn <- c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5")
```
   
# (a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

#### Logistic
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("glm",timagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")#First fold method
a2 <- CVmodel_accuracy("glm",imagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Transformed images have better CV accuracy, so use second split on test data
model <- glm("cl ~ NDAI + CORR +SD", data=timagestrain, family="binomial")
model.pred <- predict(model, timagestest, type="response")
datclassedglm <- rep(0, length(model.pred))
datclassedglm[model.pred >= 0.5] = 1
zero_one_loss(datclassedglm, timagestest$cl)
table(datclassedglm, timagestest$cl)
```

#### K Nearest Neighbors
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("knn",timagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("knn",imagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
#Test Accuracy- Untransformed images have slightly better CV accuracy but lower test accuracy
datclassedknn <- knn(imagestrain[,4:6], imagestest[,4:6],imagestrain$cloud_label,k=10)
zero_one_loss(datclassedknn, imagestest$cloud_label)
table(datclassedknn, imagestest$cloud_label)
```
#### LDA 
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("lda",timagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("lda",imagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Untransformed images have better slightly better CV and test accuracy, 
#so use second split on test data
model <- lda(cloud_label ~ NDAI + CORR +SD,timagestrain)
model.pred <- predict(model, timagestest)
datclassedlda <- model.pred$class
zero_one_loss(datclassedlda, timagestest$cloud_label)
table(datclassedlda, timagestest$cloud_label)
```

#### QDA
```{r}
a1 <- CVmodel_accuracy("qda",timagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("qda",imagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Transformed data has huigher CV accuracy 
model <- qda(cloud_label ~ NDAI + CORR +SD,timagestrain)
model.pred <- predict(model, timagestest)
datclassedqda <- model.pred$class
zero_one_loss(datclassedqda, timagestest$cloud_label)
table(datclassedqda, timagestest$cloud_label)
```


```{r}
# library(caret)
# confusionMatrix(as.factor(first_pred), as.factor(log_imagesval$cloud_label))
# 
# library(DAAG)
# CVbinary(first_log)
# # I ran the above CVbinary part a few times to check consistency of the results. The accuracy is always 0.893, which is higher than 0.8884 which is what I was getting without Cross-validation. 
# confusionMatrix(as.factor(first_lda.class), as.factor(lda_imagesval$cloud_label))
# # 89.74% accuracy running lda on untransformed data
# confusionMatrix(as.factor(second_lda.class), as.factor(tlda_imagesval$cloud_label))
# #90.9% accurary running LDA on transformed data 
# confusionMatrix(as.factor(first_qda.class), as.factor(qda_imagesval$cloud_label))
# # 89.81% accuracy running qda on untransformed data
```
# (b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.
```{r}

first_lda.pred$posterior[,2]
first_lda_pred <- prediction(as.double(first_lda.pred$posterior[,2]),
                             lda_imagesval$cloud_label)
first_lda_perf <- performance(first_lda_pred, "tpr", "fpr")
plot(first_lda_perf, colorize=TRUE) 
```

```{r}
library(ROCR)
par(mfrow=c(1,1))

#lda first
first_lda_pred <- prediction(as.double(first_lda.class), lda_imagesval$cloud_label)
first_lda_perf <- performance(first_lda_pred, "tpr", "fpr")
plot(first_lda_perf, colorize=TRUE) 

#lda second
second_lda_pred <- prediction(as.double(second_lda.class), tlda_imagesval$cloud_label)
second_lda_perf <- performance(second_lda_pred, "tpr", "fpr")
plot(second_lda_perf, colorize=TRUE)

#qda first
first_qda_pred <- prediction(as.double(first_qda.class), qda_imagesval$cloud_label)
fist_qda_perf <- performance(first_qda_pred, "tpr", "fpr")
plot(fist_qda_perf, colorize=TRUE)

#qda second 
second_qda_pred <- prediction(as.double(second_qda.class), tqda_imagesval$cloud_label)
second_qda_perf <- performance(second_qda_pred, "tpr", "fpr")
plot(second_qda_perf, colorize=TRUE)

#logistic first 
first_log_pred <- prediction(as.double(first_pred), log_imagesval$cloud_label)
first_log_perf <- performance(first_log_pred, "tpr", "fpr")
plot(first_log_perf, colorize=TRUE)

#logistic second
second_log_pred <- prediction(as.double(second_pred), tlog_imagesval$cloud_label)
second_log_perf <- performance(second_log_pred, "tpr", "fpr")
plot(second_log_perf, colorize=TRUE)

#knn first
first_knn_pred <- prediction(as.double(first_KNN[[10]]), knn_imagesval$cloud_label)
first_knn_perf <- performance(first_knn_pred, "tpr", "fpr")
plot(first_knn_perf, colorize=TRUE)

#knn second
second_knn_pred <- prediction(as.double(second_KNN[[10]]), tknn_imagesval$cloud_label)
second_knn_perf <- performance(second_knn_pred, "tpr", "fpr")
plot(second_knn_perf, colorize=TRUE)

# save all the predicted values and true values into two lists

```


# (c) (Bonus) Assess the fit using other relevant metrics.

