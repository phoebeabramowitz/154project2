---
title: "Modeling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(DAAG)
library(class)
library(ROCR)
library(gbm)
library(adabag)
```

```{r}

CVmodel_accuracy <- function(classifier, data, K, loss, featstrain, labeltrain){

  #still the error
  folds <- createFolds(data$cloud_label, k = K)
  fold_loss <- rep(0,K)
  
  for (i in 1:K){
    val_dat <- data[folds[[i]],]
    train_dat <- data[-folds[[i]],]
    
    #knn
    if (classifier=="knn"){
      datclassed <- knn(train_dat[,4:6], val_dat[,4:6],train_dat$cloud_label,k=10)
    }
    
    #logistic
    else if (classifier=="glm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- glm(formula, data=train_dat, family="binomial")
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    
    #lda
    else if(classifier=="lda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- lda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #qda
    else if(classifier=="qda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- qda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #boosting
    else if(classifier=="gbm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- gbm(formula, data = train_dat, distribution = "gaussian")
      model.pred <- predict(model, val_dat, n.trees = 100)
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }
    #This part is the problem of QDA
    fold_loss[i] <- loss(datclassed, val_dat$cloud_label) 
  }  
  return(fold_loss)
}

CVmodel_accuracy("gbm",timagestrain,10,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")

```

## 3. We now try to fit different classification models and assess the fitted models using different criterion. For the next three parts, we expect you to try logistic regression and at least three other methods. 
  
# (a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

```{r}
# logistic regression only works with values between 0 and 1, so I removed unlabled points from the first split sets, and then replaced -1 with 0.

log_imagestrain <- imagestrain %>% filter(cloud_label != 0)
log_imagesval <- imagesval %>% filter(cloud_label != 0)
log_imagestest <- imagestest %>% filter(cloud_label != 0)

log_imagestrain$cloud_label <- replace(log_imagestrain$cloud_label, 
                                       log_imagestrain$cloud_label == -1, 
                                       0)

log_imagestest$cloud_label <- replace(log_imagestest$cloud_label, 
                                       log_imagestest$cloud_label == -1, 
                                       0)

log_imagesval$cloud_label <- replace(log_imagesval$cloud_label, 
                                       log_imagesval$cloud_label == -1, 
                                       0)

# logistic regression on first split has 88% accuracy on the validation set


first_log <- glm(cloud_label ~ NDAI + CORR + SD, data=log_imagestrain, family = binomial)

first_log.pred <- predict(first_log, log_imagesval, type="response")
first_pred <- rep(0, length(first_log.pred))
first_pred[first_log.pred >= 0.5] = 1

mean(first_pred == log_imagesval$cloud_label) # this calculates accuracy

# Run termplot(first_log, smooth=panel.smooth) in console to confirm all covariates (NDAI, CORR, SD) are linear 

#compare the accuracy using the 'caret' package..
#diagonal elements of the confusion matrix indicate correct predictions. offdiagonal are incorrect predictions
library(caret)
confusionMatrix(as.factor(first_pred), as.factor(log_imagesval$cloud_label))

library(DAAG)
CVbinary(first_log)
# I ran the above CVbinary part a few times to check consistency of the results. The accuracy is always 0.893, which is higher than 0.8884 which is what I was getting without Cross-validation. 
```
```{r}
#run logistic regression again, but with transformed data
tlog_imagestrain <- timagestrain %>% filter(cloud_label != 0)
tlog_imagesval <- timagesval %>% filter(cloud_label != 0)
tlog_imagestest <- timagestest %>% filter(cloud_label != 0)

tlog_imagestrain$cloud_label <- replace(tlog_imagestrain$cloud_label, 
                                       tlog_imagestrain$cloud_label == -1, 
                                       0)

tlog_imagestest$cloud_label <- replace(tlog_imagestest$cloud_label, 
                                       tlog_imagestest$cloud_label == -1, 
                                       0)

tlog_imagesval$cloud_label <- replace(tlog_imagesval$cloud_label, 
                                       tlog_imagesval$cloud_label == -1, 
                                       0)

second_log <- glm(cloud_label ~ NDAI + CORR + SD, data=tlog_imagestrain, family = binomial)

second_log.pred <- predict(second_log, tlog_imagesval, type="response")
second_pred <- rep(0, length(second_log.pred))
second_pred[second_log.pred > 0.5] = 1

mean(second_pred == tlog_imagesval$cloud_label)

confusionMatrix(as.factor(second_pred), as.factor(tlog_imagesval$cloud_label))

CVbinary(second_log)

# The normal logistic regression accuracy is higher here than the CV accuracy...interesting
```

```{r}
# run lda on the untransformed data..

lda_imagestrain <- imagestrain %>% filter(cloud_label != 0)
lda_imagesval <- imagesval %>% filter(cloud_label != 0)
lda_imagestest <- imagestest %>% filter(cloud_label != 0)

first_lda <- lda(cloud_label ~ NDAI + CORR + SD, data = lda_imagestrain)

first_lda.pred <- predict(first_lda, lda_imagesval)
first_lda.class <- first_lda.pred$class

mean(first_lda.class == lda_imagesval$cloud_label)

confusionMatrix(as.factor(first_lda.class), as.factor(lda_imagesval$cloud_label))
# 89.74% accuracy running lda on untransformed data
```

```{r}
# run lda on transformed data 
tlda_imagestrain <- timagestrain %>% filter(cloud_label != 0)
tlda_imagesval <- timagesval %>% filter(cloud_label != 0)
tlda_imagestest <- timagestest %>% filter(cloud_label != 0)

second_lda <- lda(cloud_label ~ NDAI + CORR + SD, data = tlda_imagestrain)

second_lda.pred <- predict(second_lda, tlda_imagesval)
second_lda.class <- second_lda.pred$class

confusionMatrix(as.factor(second_lda.class), as.factor(tlda_imagesval$cloud_label))
#90.9% accurary running LDA on transformed data 
```

```{r}
# qda on untransformed data
qda_imagestrain <- imagestrain %>% filter(cloud_label != 0)
qda_imagesval <- imagesval %>% filter(cloud_label != 0)
qda_imagestest <- imagestest %>% filter(cloud_label != 0)

first_qda <- qda(cloud_label ~ NDAI + CORR + SD, data = qda_imagestrain)

first_qda.pred <- predict(first_qda, qda_imagesval)
first_qda.class <- first_qda.pred$class

confusionMatrix(as.factor(first_qda.class), as.factor(qda_imagesval$cloud_label))
# 89.81% accuracy running qda on untransformed data
```

```{r}
# qda on transformed data 

tqda_imagestrain <- timagestrain %>% filter(cloud_label != 0)
tqda_imagesval <- timagesval %>% filter(cloud_label != 0)
tqda_imagestest <- timagestest %>% filter(cloud_label != 0)

second_qda <- qda(cloud_label ~ NDAI + CORR + SD, data = qda_imagestrain)

second_qda.pred <- predict(second_qda, tqda_imagesval)
second_qda.class <- second_qda.pred$class

confusionMatrix(as.factor(second_qda.class), as.factor(tqda_imagesval$cloud_label))
# 90.14% accuracy running qda on transformed data
```


# (b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.

```{r}
library(ROCR)
par(mfrow=c(1,1))

#lda first
first_lda_pred <- prediction(as.double(first_lda.class), lda_imagesval$cloud_label)
first_lda_perf <- performance(first_lda_pred, "tpr", "fpr")
plot(first_lda_perf, colorize=TRUE) 

#lda second
second_lda_pred <- prediction(as.double(second_lda.class), tlda_imagesval$cloud_label)
second_lda_perf <- performance(second_lda_pred, "tpr", "fpr")
plot(second_lda_perf, colorize=TRUE)

#qda first
first_qda_pred <- prediction(as.double(first_qda.class), qda_imagesval$cloud_label)
fist_qda_perf <- performance(first_qda_pred, "tpr", "fpr")
plot(fist_qda_perf, colorize=TRUE)

#qda second 
second_qda_pred <- prediction(as.double(second_qda.class), tqda_imagesval$cloud_label)
second_qda_perf <- performance(second_qda_pred, "tpr", "fpr")
plot(second_qda_perf, colorize=TRUE)

#logistic first 
first_log_pred <- prediction(as.double(first_pred), log_imagesval$cloud_label)
first_log_perf <- performance(first_log_pred, "tpr", "fpr")
plot(first_log_perf, colorize=TRUE)

#logistic second
second_log_pred <- prediction(as.double(second_pred), tlog_imagesval$cloud_label)
second_log_perf <- performance(second_log_pred, "tpr", "fpr")
plot(second_log_perf, colorize=TRUE)

#knn first
first_knn_pred <- prediction(as.double(first_KNN[[10]]), knn_imagesval$cloud_label)
first_knn_perf <- performance(first_knn_pred, "tpr", "fpr")
plot(first_knn_perf, colorize=TRUE)

#knn second
second_knn_pred <- prediction(as.double(second_KNN[[10]]), tknn_imagesval$cloud_label)
second_knn_perf <- performance(second_knn_pred, "tpr", "fpr")
plot(second_knn_perf, colorize=TRUE)

# save all the predicted values and true values into two lists

```


# (c) (Bonus) Assess the fit using other relevant metrics.

