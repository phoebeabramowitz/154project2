---
title: "Preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(reshape2)
```

# (a) Data Split: Split the entire data (image1.txt, image2.txt, image3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

Two ways:
1) to just split the datasets into 80% train, 10% validation, and 10% test without any transformations.

Should we combine before we split?
```{r}
set.seed(1997)
spec = c(train = .8, test = .1, val = .1)
splitter <- function(df){
  g = sample(cut(
    seq(nrow(df)), 
    nrow(df)*cumsum(c(0,spec)),
    labels = names(spec)))
  res = split(df,g)
}
images_comb <- rbind(image1, image2, image3)
isplit <- splitter(images_comb)
#delete unlabelled\
isplit <- filter(isplit,cloud_label != 0)
imagestrain <- isplit$train
imagesval <- isplit$val
imagestest <- isplit$test


#removing zeros improves trivial classifier accuracy from 37% to 61%
# imagestrain <- imagestrain %>% filter(cloud_label != 0)
# imagesval <- imagesval %>% filter(cloud_label != 0)
# imagestest <- imagestest %>% filter(cloud_label != 0)

# i1split <- splitter(image1)
# i2split <- splitter(image2)
# i3split <- splitter(image3)
# # Entire data(all 3 images) split into three sets
# imagestrain <- rbind(i1split$train, i2split$train,i3split$train)
# imagesval <- rbind(i1split$val, i2split$val,i3split$val)
# imagestest <- rbind(i1split$test, i2split$test,i3split$test)
```

2) take 3pixel X 3pixel blocks and creating 'super pixels' to transform the data into a new set. This will decrease the number of data points, but should help reconcile the dependence relationship between the points.

```{r}
##Ignore all of these notes, just for me
#establish which pixels are next to each other in 3x3 pixel blocks
#idea: Start in lower left corner and iterate(while or for loop?)  until all pixels are grouped
#image ranges: 
#y(all): 2-383 with no gaps(382)
###1 is x:65-369, with no gaps(305)   

###2 is x:65-368, no gaps(304) 

###3 is 65-369, no gaps(305)

#last case/outer region might be uneven, how to deal with that?
#rename the x and y consecutivley, not on average(so we can do mapping thing) 
#Average of three points for cloud_label: nonzero = cloud, <0= no cloud
```

```{r}
#cloud_avg function takes in 9 values(vector) and returns -1, 0, 1 depending on avg
cloud_avg <- function(vector){
  avg <- mean(vector)
  if(avg<0){
    return(-1)}
  else if(avg>0){
    return(1)}
  else{
    return(0)}
}

#' Super Pixelize
#'
#' @param images list of dataframes with correct column names
#'
#' @return timages, a data frame in the global environment with 1/9th the values, averaged by 3x3 blocks,no x,y cols
# super_pixelize <- function(images){
#   newimages <-data.frame(matrix(ncol = 9, nrow = 0))
#   for(k in 1:length(images)){
#     image <- images[[k]]
#     xs <- seq(from=min(image$x), to=max(image$x), by=3)
#     ys <- seq(from=min(image$y), to=max(image$y), by=3)
#     ktrans <-data.frame(matrix(ncol = 9, nrow = 0))
#     for(i in xs){
#      for(j in ys){
#         pts <- image %>%filter((x==i | x==i+1 | x== i+2)&(y==j | y==j+1 | y== j+2))
#         if (nrow(pts)>0){
#           new_row <- c(cloud_avg(pts$cloud_label),mean(pts$NDAI),
#                       mean(pts$SD),mean(pts$CORR),mean(pts$rad_DF),mean(pts$rad_CF),
#                       mean(pts$rad_BF),mean(pts$rad_AF),mean(pts$rad_AN))
#           ktrans <- rbind(ktrans, new_row)
#         }
#       }
#     }
#     newimages <- rbind(newimages, ktrans)
#   }  
#   images_columns <- c("cloud_label","NDAI","SD","CORR","rad_DF","rad_CF",
#                     "rad_BF","rad_AF","rad_AN" )
#   colnames(newimages) <- images_columns 
#   return(newimages)
# }
super_pixelize <- function(images){
  newimages <-data.frame(matrix(ncol = 9, nrow = 0))
  for(k in 1:length(images)){
    image <- images[[k]]
    xs <- seq(from=min(image$x), to=max(image$x), by=3)
    ys <- seq(from=min(image$y), to=max(image$y), by=3)
    ktrans <-data.frame(matrix(ncol = 9, nrow = 0))
    for(i in xs){
      for(j in ys){
        pts <- image %>%filter((x==i | x==i+1 | x== i+2)&(y==j | y==j+1 | y== j+2))
        if (nrow(pts)>0){
          new_row <- c(cloud_avg(pts$cloud_label),mean(pts$NDAI),
                      mean(pts$SD),mean(pts$CORR),mean(pts$rad_DF),mean(pts$rad_CF),
                      mean(pts$rad_BF),mean(pts$rad_AF),mean(pts$rad_AN))
          ktrans <- rbind(ktrans, new_row)
         }
      }
    }
    images_columns <- c("cloud_label","NDAI","SD","CORR","rad_DF","rad_CF",
                    "rad_BF","rad_AF","rad_AN" )
    colnames(newimages) <- images_columns
    colnames(ktrans) <- images_columns 
    newimages <- rbind(newimages,ktrans)
  }

  return(newimages)
}

pics <- list(image1,image2,image3)
timages <- super_pixelize(pics)
 
```
Then, delete every unlabelled point and split(Add this in before for cheaper function)
```{r}
# timages <- filter(timages,cloud_label != 0)
tisplit <- splitter(timages)
#delete unlabelled
timagestrain <- tisplit$train
timagesval <- tisplit$val
timagestest <- isplit$test

```

# (b) Baseline: Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

```{r, warning=FALSE}
trivial <- rep(-1, nrow(imagesval))
ttrivial1 <- rep(-1, nrow(timagestest))
ttrivial2 <- rep(-1, nrow(timagesval))

data.frame(
  data_set = c("first_split_val", "first_split_test", "second_split_val", "second_split_test"),
  trivial_classifier_accuracy = c(mean(imagesval$cloud_label == trivial),
                                  mean(imagestest$cloud_label == trivial),
                                  mean(timagesval$cloud_label == ttrivial2),  
                                  mean(timagestest$cloud_label == ttrivial1)))
```

A trivial classifier has about a 37%-38% accuracy on both the validation and test data sets. This kind of classifier will have high average accuracy if the actual images depict mostly cloudless regions. 

# (c) First order importance: Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

The farther the spread between the two distributions (least overlap) the better. We expect the distribution of cloud vs no clouds to be different, so NDAI seems to be the 'best' feature for distinguishing between the two. This simplifies to a binary classification problem; similar to a decision stump, we can use NDAI as a feature to train our classifier since it has small error (small overlap of histograms). We can asses this error directly in ROC curves, where we compare true positive, and false positive rates. 
```{r}

# new1 <- image1 %>% filter(cloud_label==1 | cloud_label==-1)
# new2 <- image2 %>% filter(cloud_label==1 | cloud_label==-1)
# new3 <- image3 %>% filter(cloud_label==1 | cloud_label==-1)
# 
# new1$cloud_label <- replace(new1$cloud_label, new1$cloud_label == 1, "Cloud")
# new1$cloud_label <- replace(new1$cloud_label, new1$cloud_label == -1, "No Cloud")

new_train <- imagestrain %>% filter(cloud_label==1 | cloud_label==-1)
new_train$cloud_label <- replace(new_train$cloud_label, new_train$cloud_label == 1, "Cloud")
new_train$cloud_label <- replace(new_train$cloud_label, new_train$cloud_label == -1, "No Cloud")


ggplot(new_train, aes(x=NDAI, fill=cloud_label)) + geom_histogram(alpha=0.2, bins=50) + theme_classic() + ggtitle("Histogram of NDAI based on cloud label: first split") 

# something is wrong here... this looks different than the first instagram below but I don't know why. If we can't figure out how to add a legend to the histograms below, we should just use these..
```

#use combined data
```{r}
## need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud

ggplot(imagestrain, aes(x=NDAI)) + 
  geom_histogram(data=subset(imagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(imagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of NDAI based on cloud label: first split") 

ggplot(timagestrain, aes(x=NDAI)) + 
  geom_histogram(data=subset(timagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(timagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) + theme_classic() + ggtitle("Histogram of NDAI based on cloud label: second split") 
```

```{r}
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud

ggplot(imagestrain, aes(x=CORR)) + 
  geom_histogram(data=subset(imagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(imagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of CORR based on cloud label: first split")

ggplot(timagestrain, aes(x=CORR)) + 
  geom_histogram(data=subset(timagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(timagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of CORR based on cloud label: second split")
```

```{r}
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud

ggplot(imagestrain, aes(x=SD)) + 
  geom_histogram(data=subset(imagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(imagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of SD based on cloud label: first split")

ggplot(timagestrain, aes(x=SD)) + 
  geom_histogram(data=subset(timagestrain,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(timagestrain,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of SD based on cloud label: second split")

# SD seems to be a better feature on the super-pixelized data set... there's less overlap in the plots
```

```{r}
# run PCA on the angles to create a 'super feature', and see how it performs when predicting cloud label

pca_angles <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=imagestrain,
                     scale. = TRUE)

loadings <- pca_angles$rotation
scores <- pca_angles$x
eigenvalues <- pca_angles$sdev^2

eigs_cum = cumsum(eigenvalues) / sum(eigenvalues)

scree_plot <- ggplot() + 
  geom_point(aes(x = 1:length(eigenvalues), y=eigs_cum)) +
  labs(x = "Principal Component", y = "Fraction of Total Variance Explained") +
  ggtitle("Screeplot") + 
  theme_minimal()

PC1 <- scores[,1]

train_data_pca <- imagestrain
train_data_pca$PC1 <- PC1

ggplot(train_data_pca, aes(x=PC1)) + 
  geom_histogram(data=subset(train_data_pca,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(train_data_pca,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of PC1 based on cloud label: first split") 

# PC1 captures 85% of the variance of all five radiance angles. It does not do a good job of predicting cloud labels however, and therefore we will not use any of the radiance angles, nor the respective principal component as a feature to train our classifier. 
```

```{r}
# try again but this time with transformed data... (i dont think it'll make a difference)

pca_angles2 <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=timagestrain,
                     scale. = TRUE)

loadings2 <- pca_angles2$rotation
scores2 <- pca_angles2$x
eigenvalues2 <- pca_angles2$sdev^2

eigs_cum2 = cumsum(eigenvalues2) / sum(eigenvalues2)

scree_plot <- ggplot() + 
  geom_point(aes(x = 1:length(eigenvalues2), y=eigs_cum2)) +
  labs(x = "Principal Component", y = "Fraction of Total Variance Explained") +
  ggtitle("Screeplot") + 
  theme_minimal()

PC1_2 <- scores2[,1]

train_data_pca2 <- timagestrain
train_data_pca2$PC1 <- PC1_2

ggplot(train_data_pca2, aes(x=PC1)) + 
  geom_histogram(data=subset(train_data_pca2,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(train_data_pca2,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of PC1 based on cloud label: second split") 
```


# (d) Write a generic cross validation (CV) function CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.
```{r}
CVgeneric <- function(classifier, features, labels, k, loss){
  #get code to divide data into k folds from previous lab/homework
  
}
```



From piazza: Clarification about CVgeneric on April 22: 
"CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K, and a loss function":

- training features/labels: here the training features/labels mean all data points except those in the test set. So your CVgeneric will run the split and the classification algorithm inside the function

- loss function: any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
      L2 loss, because there's no need to induce sparsity, or exponential loss, etc. 


```{r}
# random tests 
a <- data.frame(A=rep(1, 100), B=rep(2,100), C=rep(3,100))
b <- data.frame(A=rep(1, 100), B=rep(2,100), C=rep(3,100))
c <- data.frame(A=rep(1, 100), B=rep(2,100), C=rep(3,100))
d <- data.frame(A=rep(1, 100), B=rep(2,100), C=rep(3,100))

df_vector <- list(c(a,b,c,d))

df_vector[[1]]

```

