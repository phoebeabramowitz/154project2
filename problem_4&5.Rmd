---
title: "Diagnostics and Reproducibility"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(scales)
```

## 4 Diagnostics
Disclaimer: The questions in this section are open-ended. Be visual and quantitative! The gold standard arguments would be able to convince National Aeronautics and Space Administration (NASA) to use your classification methodâ€”in which case Bonus points will be awarded.


# (a) Do an in-depth analysis of a good classification model of your choice by showing some diagnostic plots or information related to convergence or parameter estimation.

  K nearest neighbors(KNN) outperformed all other classifiers we tried, and has a higher accuracy using the transformed data set. In part three we only used NDAI, CORR, and SD to train our models. Therefore, in this section we explore KNN accuracy varied over different training features and increasing k values.
  We see an increase in model accuracy when we add more radiance angles as features. Moreover, we see a substantial increase in model accuracy we run PCA on the radiance angles and use the first PC as a feature. In part two of the project we deduced that PC1 of the radiance angles was not a good feature since the histograms of cloud label were largely overlapping. Hoever, we see from this analysis that it does improve the model accuracy for K-nearest neighbors significantly. 
  It is clear that highest model accuracy for KNN occurs for k=10, which is what we used throughout the project. 
```{r}
tpca_train <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=timagestrain,
                     scale. = TRUE)

tscores_train <- tpca_train$x
tPC1_train <- tscores_train[,1]
timagestrain$PC1 <- tPC1_train

tpca_test <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=timagestest,
                     scale. = TRUE)

tscores_test <- tpca_test$x
tPC1_test <- tscores_test[,1]
timagestest$PC1 <- tPC1_test
```


```{r}
second_KNN <- list()
tfour_feats <- list()
tfive_feats <- list()
pca_knn <- list()
for (i in 1:10){
  #All Second Split Method(Transformed)
  second_KNN[[i]] <- knn(timagestrain[,2:4], timagestest[,2:4], 
                         timagestrain$cloud_label, i)
  tfour_feats[[i]] <- knn(timagestrain[,2:5], timagestest[,2:5],
                        timagestrain$cloud_label, i)
  tfive_feats[[i]] <- knn(timagestrain[,c(2:5,9)], timagestest[,c(2:5,9)],
                        timagestrain$cloud_label, i)
  pca_knn[[i]] <- knn(timagestrain[,c(2:4,12)], timagestest[,c(2:4,11)],
                      timagestrain$cloud_label, i)
  
}

second_accuracy <- list()
fourf_accuracy <- list()
fivef_accuracy <- list()
pca_accuracy <- list()
for (i in 1:10){
  second_accuracy[[i]] <- zero_one_loss(second_KNN[[i]], timagestest$cloud_label)
  fourf_accuracy[[i]] <- zero_one_loss(tfour_feats[[i]], timagestest$cloud_label)
  fivef_accuracy[[i]] <- zero_one_loss(tfive_feats[[i]], timagestest$cloud_label)
  pca_accuracy[[i]] <- zero_one_loss(pca_knn[[i]], timagestest$cloud_label)
}

knndf <- data.frame(K = c(1,2,3,4,5,6,7,8,9,10), 
                Three_Features = unlist(second_accuracy),
                Four_Features = unlist(fourf_accuracy),
                Five_Features = unlist(fivef_accuracy),
                PCA = unlist(pca_accuracy))
knndftall1 <- melt(knndf, id = "K")

# ggplot((filter(knndftall1,variable != "untransformed_accuracy")), aes(x=K, y=value,color=variable)) +
ggplot(knndftall1, aes(x=K, y=value,color=variable)) +
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  ggtitle("KNN Accuracy by K Value")+
  ylab("Accuracy on Transformed Data")
  
 
```

```{r}
#Add PCA column for later
pca_train <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=imagestrain,
                     scale. = TRUE)

scores_train <- pca_train$x
PC1_train <-scores_train[,1]
imagestrain$PC1 <- PC1_train

pca_test <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=imagestest,
                     scale. = TRUE)

scores_test <- pca_test$x
PC1_test <- scores_test[,1]
imagestest$PC1 <- PC1_test
```


# (b) For your best classification model(s), do you notice any patterns in the misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in specific ranges of feature values?

First, we show the ROC curve to demonstrate the tradeoff in errors.
```{r}
prob <- attr(datclassedknn, "prob")
prob <- 2*ifelse(datclassedknn == "-1", 1-prob, prob) - 1
knn_pred <- prediction(prob, imagestest$cloud_label)
knn_perf <- performance(knn_pred, "tpr", "fpr")
plot(knn_perf, avg= "threshold", colorize=T, lwd=2, main="KNN ROC curve")
```

Given that the data is spatially dependent, it's useful to see the locations of the two types of errors on the testing subset of the three images.
```{r}
eimage1 <- mutate(image1, image=1)
eimage2 <- mutate(image2, image=2)
eimage3 <- mutate(image3, image=3)
eimages <- split1(list(eimage1,eimage2,eimage3),spec = c(train = .8, test = .2))
eimagestrain <- eimages$train
eimagestest <- eimages$test
##Add PCA
eimagestest$PC1 <- PC1_test
eimagestrain$PC1 <- PC1_train

eknnclasses <- knn(eimagestrain[,c(2:4,10)] , eimagestest[,c(2:4,10)],eimagestrain$cloud_label,k=8)
eknnclasses <- ifelse(as.double(eknnclasses)==1,-1,1)
imtesterrors <- mutate(eimagestest, error_type =(eimagestest$cloud_label - eknnclasses))
#partial heatmap image1
ggplot(filter(imtesterrors,image==1))+
  geom_tile(aes(x=x, y=y, fill=factor(error_type)))+
  labs(fill= "Error Type",
       title="Errors on Test Subset: Image1")+
  theme_minimal()+
  scale_fill_manual(values=c("red","lightgrey","blue"),
                   labels=c("False Negative", "Accurate", "False Positive"))

ggplot(filter(imtesterrors,image==2))+
  geom_tile(aes(x=x, y=y, fill=factor(error_type)))+
  labs(fill= "Error Type",
       title="Errors on Test Subset: Image2")+
  theme_minimal()+
  scale_fill_manual(values=c("red","lightgrey","blue"),
                   labels=c("False Negative", "Accurate", "False Positive"))

ggplot(filter(imtesterrors,image==3))+
  geom_tile(aes(x=x, y=y, fill=factor(error_type)))+
  labs(fill= "Error Type",
       title="Errors on Test Subset: Image3")+
  theme_minimal()+
  scale_fill_manual(values=c("red","lightgrey","blue"),
                   labels=c("False Negative", "Accurate", "False Positive"))

```
The vast majority of the area in our model for all three images is error free. It's clear from the above images that errors are clustered in certain clouds or cloudless regions. Furthermore, those spatially clustered errors are of the same type(i.e. there are clouds that have relatively high rates of being mislabelled as no cloud, a few small cloudless regions with high rates of being mislabelled as a cloud). 
It's worth considering what could be making our model perform worse with these particular regions of the data. Thus, we examine the relationship between errors and certain features.

```{r}
just_errors <- filter(imtesterrors,error_type!=0)

ggplot(just_errors)+
  geom_histogram(aes(x=NDAI,fill="Errors"),alpha=0.3, bins=50)+
  aes(y=stat(count)/sum(stat(count))) + 
  geom_histogram(data=imagestest,aes(x=NDAI,fill="Overall"), alpha=0.3, bins=50)+
  scale_y_continuous(labels = scales::percent)+
  ylab("Percentage of Erroneous Points in this NDAI Bin")+
  ggtitle("Distribution of NDAI: Where our Model Gives an Error vs Overall") +
  theme_minimal()+
  scale_fill_manual(name="Error?",values=c(Errors="red", Overall="blue"))

ggplot(just_errors)+
  geom_histogram(aes(x=CORR,fill="Errors"),alpha=0.3,bins=50)+
  aes(y=stat(count)/sum(stat(count))) + 
  geom_histogram(data=imagestest,aes(x=CORR,fill="Overall"), alpha=0.3,bins=50)+
  scale_y_continuous(labels = scales::percent)+
  ylab("Percentage of Points in this CORR Bin")+
  ggtitle("Distribution of CORR: Where our Model Gives an Error vs Overall")+
  theme_minimal()+
  scale_fill_manual(name="Error?",values=c(Errors="red", Overall="blue"))

ggplot(just_errors)+
  geom_histogram(aes(x=SD,fill="Errors"),alpha=0.3,bins=50)+
  aes(y=stat(count)/sum(stat(count))) + 
  geom_histogram(data=imagestest,aes(x=SD,fill="Overall"), alpha=0.3,bins=50)+
  scale_y_continuous(labels = scales::percent)+
  ylab("Percentage of Erroneous Points in this SD Bin")+
  ggtitle("Distribution of SD: Where our Model Gives an Error vs Overall")+
  theme_minimal()+
  scale_fill_manual(name="Error?",values=c(Errors="red", Overall="blue"))
```

The distribution of NDAI values is substantially different among values where our model misclassifies than in the general test set. It appears that our model fails at much higher rates with higher NDAI values. Although this in no way shows causality, it would be valuable to consult with experts about the possible influence of NDAI on certain regions where our model has a relatively high rate failure. The distributions for SD and CORR are similar here. 



# (c)can you think of a better classifier? How well do you think your model will work on future data without expert labels?

```{r}
#clunky
boost_imagestrain <- imagestrain[,1:11]
boost_imagestest <- imagestest[,1:11]
tboost_imagestrain <- timagestrain[,1:9]
tboost_imagestest <- timagestest[,1:9]

boost_imagestrain$cloud_label <- replace(boost_imagestrain$cloud_label, 
                                       boost_imagestrain$cloud_label == -1, 
                                       0)

boost_imagestest$cloud_label <- replace(boost_imagestest$cloud_label, 
                                       boost_imagestest$cloud_label == -1, 
                                       0)

tboost_imagestrain$cloud_label <- replace(tboost_imagestrain$cloud_label, 
                                       tboost_imagestrain$cloud_label == -1, 
                                       0)

tboost_imagestest$cloud_label <- replace(tboost_imagestest$cloud_label, 
                                       tboost_imagestest$cloud_label == -1, 
                                       0)
```

```{r}

a1 <- CVmodel_accuracy("gbm", tboost_imagestrain, 5, zero_one_loss, ".", "cloud_label")
a2 <- CVmodel_accuracy("gbm", boost_imagestrain, 5, zero_one_loss, ".", "cloud_label")

data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)

# getting higher CV accuracy on untransformed data when boosting, so ill run boosting on untransformed test data
```

```{r}

boost_test <- gbm(cloud_label ~.,
                  data = boost_imagestrain,
                  distribution = "adaboost")

boost.pred <- predict(boost_test, boost_imagestest, n.trees = 100)
boost_pred <- rep(0, length(boost.pred))
boost_pred[boost.pred >= 0.5] = 1

data.frame("boosting_test_accuracy"=zero_one_loss(boost_pred, boost_imagestest$cloud_label))

# boosting is getting 90.83% accuracy on test set (untransformed)
```

## Boosting ROC
```{r}
par(mfrow=c(1,1))
boost_roc <- prediction(boost.pred, boost_imagestest$cloud_label)
boost_perf <- performance(boost_roc, "tpr", "fpr")
plot(boost_perf, colorize=TRUE, lwd=2, main="Boosting ROC curve")
```

  Boosted regression trees is one classifier we used to improve model accuracy. Boosting fits the decision trees on a modified version of the original data set, growing them sequentially and improving. This boosted approach learns slowly, and uses errors from previous trees when training the next ones. In general this approach should perform well, but we see in our data that is has a slightly lower test accuracy than KNN. 

  It's possible that our model will work poorly on future data with expert labels, if there exist factors in future data that cause the cloudiness of a region to have a different relationship with our features, than it has in the three images we trained our model on. Ideally, we would train our model on more images, as they do in the paper, to account for possible discrepancies between images and their respective polar regions. 

# (d) Do your results in parts 4(a) and 4(b) change as you modify the way of splitting the data?

```{r}
#Create Tall Dataframe for ggplot with both splits

first_pca <- list()
untransformed_accuracy <- list()
for (i in 1:10){
  first_pca[[i]] <- knn(imagestrain[,c(4:6,14)], imagestest[,c(4:6,13)],
                      imagestrain$cloud_label, i)
  untransformed_accuracy[[i]] <- zero_one_loss(first_pca[[i]], imagestest$cloud_label)
}

knndfsplits <- data.frame(K = c(1,2,3,4,5,6,7,8,9,10), 
                          PCA_untransformed_accuracy = unlist(untransformed_accuracy), 
                          PCA_transformed_accuracy = unlist(pca_accuracy))

knndfsplitstall <- melt(knndfsplits, id = "K")

ggplot((knndfsplitstall), aes(x=K, y=value, color=variable)) +
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  ggtitle("KNN Accuracy by K Value, Both Splits")

```

  After discovering the improved accuracy of adding PC1 of radiance angles as a training feature, we check to see whether there is a noticeable difference between the transformed and untransformed data on model accuracy. We see above the difference is slight, but the transformed data does outperform the untransformed data. 


# (e) Write a paragraph for your conclusion.
  In this report we trained five separate classification models to predict cloud labels in the artic region, without deep learning or neural networks. We present the power of classic statistical prediction, with deliberately chosen training features to fit our model. An analysis of the model accuracies was completed to compare the performance of each method. Lastly, we run several diagnostic procedures to asses how to improve our models, as well as explore misclassified results. 


## 5 Reproducibility
In addition to a writeup of the above results, please provide a one-line link to a public GitHub repository containing everything necessary to reproduce your writeup. Specifically, imagine that at some point an error is discovered in the three image files, and a future researcher wants to check whether your results hold up with the new, corrected image files. This researcher should be able to easily re-run all your code and produce all your figures and tables. This repository should contain:

(i) The pdf of the report,
(ii) the raw Latex, Rnw or Word used to generate your report,
(iii) your R code (with CVgeneric function in a separate R file),
(iv) a README file describing, in detail, how to reproduce your paper from scratch (assume researcher has access to the images).

 We need two things:
(a) A main pdf report (font size at least 11 pt, less or equal to 12 pages)
generated by Latex, Rnw or Word is required to be submitted to Gradescope.
â€“ Provide top class (research-paper level) writing, useful well-labeled figures and no code in this pdf. Arrange text and figures compactly (.Rnw may not be very useful for this).
â€“ ***You can choose a title for the report and a team name as per your liking (get creative!). Do provide the names and student ID of your teammates below the title.***
â€“ Your report should conclude with an acknowledgment section, where you provide brief discussion about the contributions of each member, and the resources you used, credit all the help you took and briefly outline the way you proceeded with the project.
(b) A link to your GitHub Repo at the end of your write-up that contains all your code (see Section 5 for more details).


#Acknowledgments

Contributions of each member:
1. a) Omri b) Phoebe c) Omri
2. a) Phoebe b) Omri c) Omri d) Phoebe
3. a) Phoebe Omri b) Omri Phoebe 
4. a) Omri b) Phoebe c) Omri d) Omri e) Phoebe
5. Omri Phoebe

resources used: 
Raaz office hours, Ruansi office hours, professor Yu office hours, ESL textbook, ISL textbook, and the following sites: 
http://uc-r.github.io/discriminant_analysis
http://cseweb.ucsd.edu/~dasgupta/papers/nn-rates.pdf
https://maths-people.anu.edu.au/~johnm/courses/mathdm/2008/pdf/r-exercisesVI.pdf
https://www.displayr.com/linear-discriminant-analysis-in-r-an-introduction/
https://tgmstat.wordpress.com/2014/01/15/computing-and-visualizing-lda-in-r/
https://rpubs.com/ifn1411/LDA
https://stackoverflow.com/questions/20197106/linear-discriminant-analysis-plot-using-ggplot2
https://stackoverflow.com/questions/14085281/multiple-roc-curves-in-one-plot-rocr
https://stackoverflow.com/questions/11741599/how-to-plot-a-roc-curve-for-a-knn-model

Brief outline of how we proceeded with the project:
  Most of this project was completed while working together, often times at Yali's Cafe in Stanley Hall. Lots of questions were answered in office hours, as well as by reading through existing Piazza queries. 


