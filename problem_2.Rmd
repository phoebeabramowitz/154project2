---
title: "Preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(reshape2)
```

# (a) Data Split: Split the entire data (image1.txt, image2.txt, image3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

Two ways:
1) to just split the datasets into 80% train, 10% validation, and 10% test without any transformations.

```{r}
spec = c(train = .8, test = .1, val = .1)
splitter <- function(df){
  g = sample(cut(
    seq(nrow(df)), 
    nrow(df)*cumsum(c(0,spec)),
    labels = names(spec)))
  res = split(df,g)
}
i1split <- splitter(image1)
i2split <- splitter(image2)
i3split <- splitter(image3)
# Entire data(all 3 images) split into three sets
imagestrain <- rbind(i1split$train, i2split$train,i3split$train)
imagesval <- rbind(i1split$val, i2split$val,i3split$val)
imagestest <- rbind(i1split$test, i2split$test,i3split$test)
```

2) take 3pixel X 3pixel blocks and creating 'super pixels' to transform the data into a new set. This will decrease the number of data points, but should help reconcile the dependence relationship between the points.

```{r}
#This is all exploratory, ignore it
#establish which pixels are next to each other in 3x3 pixel blocks
#idea: Start in lower left corner and iterate(while or for loop?)  until all pixels are grouped
#image ranges: 
#y(all): 2-383 with no gaps(382)
###1 is x:65-369, with no gaps(305)   
length(unique(image1$x))
range(image1$y)
###2 is x:65-368, no gaps(304) 
unique(image2$x)
###3 is 65-369, no gaps(305)
length(unique(image3$x))

#last case/outer region might be uneven



```


  
# (b) Baseline: Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

This trivial classifier will have high average accuracy if the actual images depict mostly cloudless regions. 
```{r}

```


This seems simple enough..

# (c) First order importance: Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

The farther the spread between the two distributions (least overlap) the better. We expect the distribution of cloud vs no clouds to be different, so NDAI seems to be the 'best' feature for distinguishing between the two.

```{r}

ggplot(image1, aes(x=NDAI)) + 
  geom_histogram(data=subset(image1,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(image1,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of NDAI count based on cloud label: image1") +
  scale_colour_manual("Labels", values=c("red",, "black"))
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud

ggplot(image2, aes(x=NDAI)) + 
  geom_histogram(data=subset(image2,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(image2,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of NDAI count based on cloud label: image2") 

ggplot(image3, aes(x=NDAI)) + 
  geom_histogram(data=subset(image3,cloud_label==1), fill="blue", alpha=0.2, bins=50) +     geom_histogram(data=subset(image3,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of NDAI count based on cloud label: image3") 

## the farther the spread between the two distributions (least overlap) the better. We expect the distribution of cloud vs no clouds to be different, so NDAI seems to be the 'best' feature for distinguishing between the two. 
```

```{r}
ggplot(image1, aes(x=CORR)) + 
  geom_histogram(data=subset(image1,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image1,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of CORR count based on cloud label: image1")

ggplot(image2, aes(x=CORR)) + 
  geom_histogram(data=subset(image2,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image2,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of CORR count based on cloud label: image2")

ggplot(image3, aes(x=CORR)) + 
  geom_histogram(data=subset(image3,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image3,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of CORR count based on cloud label: image3")
```

```{r}
ggplot(image1, aes(x=SD)) + 
  geom_histogram(data=subset(image1,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image1,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of SD count based on cloud label: image1")

ggplot(image2, aes(x=SD)) + 
  geom_histogram(data=subset(image2,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image2,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of SD count based on cloud label: image2")

ggplot(image3, aes(x=SD)) + 
  geom_histogram(data=subset(image3,cloud_label==1), fill="blue", alpha=0.2, bins=50) + geom_histogram(data=subset(image3,cloud_label==-1), fill="green", alpha=0.2, bins=50) +
  theme_classic() + ggtitle("Histogram of SD count based on cloud label: image3")
```



# (d) Write a generic cross validation (CV) function CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.


From piazza: Clarification about CVgeneric on April 22: 
"CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K, and a loss function":

- training features/labels: here the training features/labels mean all data points except those in the test set. So your CVgeneric will run the split and the classification algorithm inside the function

- loss function: any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
      L2 loss, because there's no need to induce sparsity, or exponential loss, etc. 
