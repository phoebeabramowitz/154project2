---
title: "Modeling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(DAAG)
library(class)
library(ROCR)
library(gbm)
```

```{r}
#Function to Run Cross Validation without splitting the data, which is computationally expensive and only needs to be performed once
CVmodel_accuracy <- function(classifier, data, K, loss, featstrain, labeltrain){

  folds <- createFolds(data$cloud_label, k = K)
  fold_loss <- rep(0,K)
  
  for (i in 1:K){
    val_dat <- data[folds[[i]],]
    train_dat <- data[-folds[[i]],]
    
    #knn
    if (classifier=="knn"){
      datclassed <- knn(train_dat[,4:6], val_dat[,4:6],train_dat$cloud_label,k=10)
    }
    
    #logistic
    else if (classifier=="glm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- glm(formula, data=train_dat, family="binomial")
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    
    #lda
    else if(classifier=="lda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- lda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #qda
    else if(classifier=="qda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- qda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #boosting
    else if(classifier=="gbm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- gbm(formula, data = train_dat, distribution = "adaboost") 
      model.pred <- predict(model, val_dat, n.trees = 100)
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }
    fold_loss[i] <- loss(datclassed, val_dat$cloud_label) 
  }  
  return(fold_loss)
}

```

## 3. We now try to fit different classification models and assess the fitted models using different criterion. For the next three parts, we expect you to try logistic regression and at least three other methods. 

Since we're using cross-validation to choose models, we merge validation set into the test set, for of 20% of the data in total. We train the data on the remaining 80% of the entire set. 
```{r}
#dataframe setup
rn <- c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5")

timagestest <- rbind(timagestest, timagesval)
imagestest <- rbind(imagestest, imagesval)

#Add cl column for models that want binary inputs
timagestrain <- mutate(timagestrain, cl=ifelse(cloud_label==1, 1, 0))
timagestest <- mutate(timagestest, cl=ifelse(cloud_label==1, 1, 0))
imagestrain <- mutate(imagestrain, cl=ifelse(cloud_label==1, 1, 0))
imagestest <- mutate(imagestest, cl=ifelse(cloud_label==1, 1, 0))


```
   
# (a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

#### Logistic
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("glm",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")#First fold method
a2 <- CVmodel_accuracy("glm",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Transformed images have better CV accuracy, so use second split on test data
model.pred <- glm("cl ~ NDAI + CORR +SD", data=timagestrain, family="binomial")
glm.pred <- predict(model.pred, timagestest, type="response")
datclassedglm <- rep(0, length(glm.pred))
datclassedglm[glm.pred >= 0.5] = 1
data.frame("logistic_test_accuracy"=zero_one_loss(datclassedglm, timagestest$cl))

```

#### K Nearest Neighbors
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("knn",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("knn",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Untransformed images have slightly better CV accuracy but lower test accuracy
datclassedknn <- knn(imagestrain[,4:6], imagestest[,4:6],imagestrain$cloud_label,k=10, prob=TRUE)
data.frame("KNN_test_accuracy"=zero_one_loss(datclassedknn, imagestest$cloud_label))
```
#### LDA 
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("lda",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("lda",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Untransformed images have better slightly better CV and test accuracy, 
#so use second split on test data
model <- lda(cloud_label ~ NDAI + CORR +SD,timagestrain)
lda.pred <- predict(model, timagestest)
datclassedlda <- lda.pred$class
data.frame("LDA_test_accuracy"=zero_one_loss(datclassedlda, timagestest$cloud_label))
```

#### QDA
```{r}
a1 <- CVmodel_accuracy("qda",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("qda",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Transformed data has huigher CV accuracy 
model <- qda(cloud_label ~ NDAI + CORR +SD,timagestrain)
qda.pred <- predict(model, timagestest)
datclassedqda <- qda.pred$class
data.frame("QDA_test_accuracy"=zero_one_loss(datclassedqda, timagestest$cloud_label))
```

Our models in order of test accuracy from lowest to highest are:
Logistic Regression(88.96%), QDA(89.83%), LDA(89.89%), K Nearest Neighbors(91.41%).
  
  LDA and Logistic regression will produce linear decision boundaries; the main difference between them exists in the way each method computes the respective coefficients. Logistic regression coefficients are computed with an estimated mean and variance from the normal distribution, while LDA coefficients are computed with maximum likelihood estimates. LDA assumes the observations are drawn from a Gaussian distribution with a common covariance matrix, and outperforms Logistic regression when these assumptions are met. If the data is not drawn from a Gaussian distribution, then Logistic will have higher accuracy.
  KNN makes no assumptions about the decision boundaries, and hence will outperform LDA and Logistic regression when the boundary is non-linear. QDA assumes a quadratic decision boundary, and can accurately model a wider range of problems as a result. QDA can perform better in the presence of less training observations, because of the assumptions it makes about the decision boundary. 
  Our LDA model outperforms Logistic regression in test accuracy by 1% on the transformed data. This leads us to believe that the Gaussian assumptions are met, and confirms our theory that transforming the data reconciles the dependence relation among the data. LDA and QDA perform similarly, while KNN outperforms all models. Thus, it is evident the decision boundaries for our classification problem are highly non-linear, and that there are a sufficient number of data points to train on in the transformed data, despite having 1/9th the original amount. 


# (b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.

```{r}
plot( lda_perf, col="purple",main="All ROC Curves")
plot(qda_perf, add = TRUE, col="red")
plot(log_perf,add = TRUE,col = "blue")
plot(knn_perf, avg= "threshold", col= "orange", add=TRUE)
legend("bottomright", legend=c("lda", "qda","logistic","knn"),
       col=c("purple", "red","blue","orange"),lty=1)
points(x=0.13,y=0.96582,pch=16,cex=1.5)
```


  We see from this ROC curve that KNN outperforms the other three classification methods. We chose a cutoff value as close to the point (0,1) as possible, because we want low false positive rates, and high true positive rates. Given the classification problem at hand, it is not imperative to predict the values correctly, for this reason we don't have to worry about picking a lower false positive rate, and can focus instead on choosing a value which will yield as high an accuracy as possible. 
  # im trying to say that this isnt a disease classification, where someone can potentially die if we misclassify...
  