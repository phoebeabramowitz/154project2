---
title: "Modeling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(DAAG)
library(class)
library(ROCR)
library(gbm)
library(adabag)
```

```{r}

CVmodel_accuracy <- function(classifier, data, K, loss, featstrain, labeltrain){

  #still the error
  folds <- createFolds(data$cloud_label, k = K)
  fold_loss <- rep(0,K)
  
  for (i in 1:K){
    val_dat <- data[folds[[i]],]
    train_dat <- data[-folds[[i]],]
    
    #knn
    if (classifier=="knn"){
      datclassed <- knn(train_dat[,4:6], val_dat[,4:6],train_dat$cloud_label,k=10)
    }
    
    #logistic
    else if (classifier=="glm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- glm(formula, data=train_dat, family="binomial")
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    
    #lda
    else if(classifier=="lda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- lda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #qda
    else if(classifier=="qda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- qda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #boosting
    else if(classifier=="gbm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- gbm(formula, data = train_dat, distribution = "adaboost") 
      #getting higher accuracy with bernoulli instead of gaussian
      #getting higher accuracy with adaboost (exponential loss)
      model.pred <- predict(model, val_dat, n.trees = 100)
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }
    #This part is the problem of QDA
    fold_loss[i] <- loss(datclassed, val_dat$cloud_label) 
  }  
  return(fold_loss)
}

```

## 3. We now try to fit different classification models and assess the fitted models using different criterion. For the next three parts, we expect you to try logistic regression and at least three other methods. 

Since we're only using CV to choose models, we merge validation set into the training set 
```{r}
#dataframe setup
rn <- c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5")

timagestrain <- rbind(timagestrain, timagesval)
imagestrain <- rbind(imagestrain, imagesval)

#Add cl column for models that want binary inputs
timagestrain <- mutate(timagestrain, cl=ifelse(cloud_label==1, 1, 0))
timagestest <- mutate(timagestest, cl=ifelse(cloud_label==1, 1, 0))
imagestrain <- mutate(imagestrain, cl=ifelse(cloud_label==1, 1, 0))
imagestest <- mutate(imagestest, cl=ifelse(cloud_label==1, 1, 0))


```
   
# (a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

#### Logistic
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("glm",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")#First fold method
a2 <- CVmodel_accuracy("glm",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Transformed images have better CV accuracy, so use second split on test data
model <- glm("cl ~ NDAI + CORR +SD", data=timagestrain, family="binomial")
glm.pred <- predict(model, timagestest, type="response")
datclassedglm <- rep(0, length(model.pred))
datclassedglm[glm.pred >= 0.5] = 1
zero_one_loss(datclassedglm, timagestest$cl)
table(datclassedglm, timagestest$cl)
```

#### K Nearest Neighbors
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("knn",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("knn",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
#Test Accuracy- Untransformed images have slightly better CV accuracy but lower test accuracy
datclassedknn <- knn(imagestrain[,4:6], imagestest[,4:6],imagestrain$cloud_label,k=10, prob=TRUE)
zero_one_loss(datclassedknn, imagestest$cloud_label)
table(datclassedknn, imagestest$cloud_label)
```
#### LDA 
```{r} 
#CV for both fold methods, across folds
a1 <- CVmodel_accuracy("lda",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("lda",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Test Accuracy- Untransformed images have better slightly better CV and test accuracy, 
#so use second split on test data
model <- lda(cloud_label ~ NDAI + CORR +SD,timagestrain)
lda.pred <- predict(model, timagestest)
datclassedlda <- lda.pred$class
zero_one_loss(datclassedlda, timagestest$cloud_label)
table(datclassedlda, timagestest$cloud_label)
```

#### QDA
```{r}
a1 <- CVmodel_accuracy("qda",timagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
a2 <- CVmodel_accuracy("qda",imagestrain,5,loss=zero_one_loss,
                 c("NDAI","CORR","SD"),"cloud_label")
data.frame("transformed"=round(a1,3),"untransformed"=round(a2,3), row.names=rn)
#Transformed data has huigher CV accuracy 
model <- qda(cloud_label ~ NDAI + CORR +SD,timagestrain)
qda.pred <- predict(model, timagestest)
datclassedqda <- qda.pred$class
zero_one_loss(datclassedqda, timagestest$cloud_label)
table(datclassedqda, timagestest$cloud_label)
```

# (b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.


## LDA ROC
```{r}
library(ROCR)

lda_pred <- prediction(lda.pred$posterior[,2], timagestest$cloud_label)
lda_perf <- performance(lda_pred, "tpr", "fpr")
plot(lda_perf, colorize=TRUE)
```

#QDA ROC
```{r}
qda_pred <- prediction(qda.pred$posterior[,2], timagestest$cloud_label)
qda_perf <- performance(qda_pred, "tpr", "fpr")
plot(qda_perf, colorize=TRUE)
```

#logistic ROC
```{r}
log_pred <- prediction(glm.pred, timagestest$cl)
log_perf <- performance(log_pred, "tpr", "fpr")
plot(log_perf, colorize=TRUE)
```


##knn ROC
```{r}
prob <- attr(datclassedknn, "prob")
prob <- 2*ifelse(datclassedknn == "-1", 1-prob, prob) - 1
knn_pred <- prediction(prob, imagestest$cloud_label)
knn_perf <- performance(knn_pred, "tpr", "fpr")
plot(knn_perf, avg= "threshold", colorize=T, lwd=3, main="KNN ROC curve")
```


# (c) (Bonus) Assess the fit using other relevant metrics.

