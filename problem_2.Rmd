---
title: "Preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# (a) Data Split: Split the entire data (image1.txt, image2.txt, image3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

Two ways:
1) to just split the datasets into 80% train, 10% validation, and 10% test without any transformations.
2) take 3pixel X 3pixel blocks and creating 'super pixels' to transform the data into a new set. This will decrease the number of data points, but should help reconcile the dependence relationship between the points.

(we could instead do the first way by taking 2x2 blocks, and the second way to take 4x4 blocks)
  Not really sure how to even do this though... Phoebe could prob help with the code. 
  
# (b) Baseline: Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

This seems simple enough..

# (c) First order importance: Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

overlaid histograms of cloud vs no cloud with x-axis as one of the features. compare the plots, and whichever have peaks closest to eachother could be considered a good feature? 
Also look at the heatmap..

```{r}

ggplot(image1, aes(x=NDAI)) + 
  geom_histogram(data=subset(image1,cloud_label==1), fill="blue", alpha=0.2, bins=40) +     geom_histogram(data=subset(image1,cloud_label==-1), fill="green", alpha=0.2, bins=40) +
  theme_classic() + ggtitle("Histogram of NDAI count based on cloud label") 
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud
```

```{r}
ggplot(image1, aes(x=CORR)) + 
  geom_histogram(data=subset(image1,cloud_label==1), fill="blue", alpha=0.2, bins=40) + geom_histogram(data=subset(image1,cloud_label==-1), fill="green", alpha=0.2, bins=40) +
  theme_classic() + ggtitle("Histogram of CORR count based on cloud label")

```

# (d) Write a generic cross validation (CV) function CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.


From piazza: Clarification about CVgeneric on April 22: 
"CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K, and a loss function":

- training features/labels: here the training features/labels mean all data points except those in the test set. So your CVgeneric will run the split and the classification algorithm inside the function

- loss function: any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
