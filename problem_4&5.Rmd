---
title: "Diagnostics and Reproducibility"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4 Diagnostics
Disclaimer: The questions in this section are open-ended. Be visual and quantitative! The gold standard arguments would be able to convince National Aeronautics and Space Administration (NASA) to use your classification method—in which case Bonus points will be awarded.


# (a) Do an in-depth analysis of a good classification model of your choice by showing some diagnostic plots or information related to convergence or parameter estimation.
We could show a partition plot of the QDA, visualizing the boundary with the partimat() function in the 'klaR' package.
If QDA yields two discriminant functions, we could make a scatterplot of the points and compare their classifications. 

Assuming we do soft-SVM and use slacking variables, we could plot the results of the classification model against different values for the slack variable.

Not too sure what to do about convergence, but maybe KNN would be good for that? 

Bias-variance plots?

```{r}
# knn on untransformed data
set.seed(1234578)

knn_imagestrain <- imagestrain[,3:6]

knn_imagesval <- imagesval[,3:6]

knn_imagestest <- imagestest[,3:6]


#knn(knn_imagestrain, knn_imagesval, knn_imagestrain$cloud_label, 2)

first_KNN <- list()
for (i in 1:10){
  first_KNN[[i]] <- knn(knn_imagestrain[,2:4], knn_imagesval[,2:4],
                        knn_imagestrain$cloud_label, i)
}

first_accuracy <- list()
for (i in 1:10){
  first_accuracy[[i]] <- mean(first_KNN[[i]] == knn_imagesval$cloud_label)
}

a <- data.frame(K = c(1,2,3,4,5,6,7,8,9,10), first_val_accuracy = unlist(first_accuracy))

# KNN on transformed data...

tknn_imagestrain <- timagestrain[,1:4]

tknn_imagesval <- timagesval[,1:4]

tknn_imagestest <- timagestest[,1:4]


#knn(knn_imagestrain, knn_imagesval, knn_imagestrain$cloud_label, 2)

second_KNN <- list()
for (i in 1:10){
  second_KNN[[i]] <- knn(tknn_imagestrain[,2:4], tknn_imagesval[,2:4], 
                         tknn_imagestrain$cloud_label, i)
}

second_accuracy <- list()
for (i in 1:10){
  second_accuracy[[i]] <- mean(second_KNN[[i]] == tknn_imagesval$cloud_label)
}

b <- data.frame(K = c(1,2,3,4,5,6,7,8,9,10), second_val_accuracy = unlist(second_accuracy))

# this diagnostic plot should be used for problem 4, but i'm going to code it here since the dataframes are in this part

c <- data.frame(K = c(1,2,3,4,5,6,7,8,9,10), first_method = unlist(first_accuracy), 
                second_method = unlist(second_accuracy))
c <- melt(c, id = "K")

ggplot(c, aes(x=K, y=value, color=variable)) +
  geom_point() + 
  geom_line() + 
  theme_minimal() +
  ggtitle("KNN validation set accuracy ")
 
```


# (b) For your best classification model(s), do you notice any patterns in the misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in specific ranges of feature values?
Make a prediction-accuracy heat map for this. There's an example for LDA classification in the link below, but I think we could figure out how to apply it to whichever model we use (https://www.displayr.com/linear-discriminant-analysis-in-r-an-introduction/)

re-run the model (knn) with x,y data, and then make a map for where the errors occur. one color of false positive, and another color for false negative and observe. 


# (c) Based on parts 4(a) and 4(b), can you think of a better classifier? How well do you think your model will work on future data without expert labels?
run random forest, or do boosting..
```{r}

CVmodel_accuracy("gbm", imagestrain, 10, zero_one_loss, ".", "cloud_label")
CVmodel_accuracy("gbm", timagestrain, 10, zero_one_loss, ".", "cloud_label")

# getting higher CV accuracy on untransformed data when boosting, so ill run boosting on untransformed test data
```

```{r}
boost_test <- gbm(cloud_label ~.,
                  data = log_imagestrain,
                  distribution = "gaussian")

boost.pred <- predict(boost_test, log_imagestest, n.trees = 100)
boost_pred <- rep(0, length(boost.pred))
boost_pred[boost.pred >= 0.5] = 1

mean(boost_pred == log_imagestest$cloud_label)

summary(boost_test)

# boosting is getting 91.7% accuracy on test set
```


# (d) Do your results in parts 4(a) and 4(b) change as you modify the way of splitting the data?
Almost all models work better on transformed data, except for boosting, which is very curious... need to write more about this, and show plots?

# (e) Write a paragraph for your conclusion.



## 5 Reproducibility
In addition to a writeup of the above results, please provide a one-line link to a public GitHub repository containing everything necessary to reproduce your writeup. Specifically, imagine that at some point an error is discovered in the three image files, and a future researcher wants to check whether your results hold up with the new, corrected image files. This researcher should be able to easily re-run all your code and produce all your figures and tables. This repository should contain:

(i) The pdf of the report,
(ii) the raw Latex, Rnw or Word used to generate your report,
(iii) your R code (with CVgeneric function in a separate R file),
(iv) a README file describing, in detail, how to reproduce your paper from scratch (assume researcher has access to the images).

 We need two things:
(a) A main pdf report (font size at least 11 pt, less or equal to 12 pages)
generated by Latex, Rnw or Word is required to be submitted to Gradescope.
– Provide top class (research-paper level) writing, useful well-labeled figures and no code in this pdf. Arrange text and figures compactly (.Rnw may not be very useful for this).
– ***You can choose a title for the report and a team name as per your liking (get creative!). Do provide the names and student ID of your teammates below the title.***
– Your report should conclude with an acknowledgment section, where you provide brief discussion about the contributions of each member, and the resources you used, credit all the help you took and briefly outline the way you proceeded with the project.
(b) A link to your GitHub Repo at the end of your write-up that contains all your code (see Section 5 for more details).


#Acknowledgments

contributions of each member:
1 a) Omri b) Phoebe c) Omri
2 a) Phoebe b) Omri c) Omri d) Phoebe
3 a) Omri b) Omri c) *do we want to do the bonus?*
4 a) b) c) d) e)
5 

resources used: 
raaz office hours, professor office hours, ESL textbook, ISL textbook, and the following sites 
http://uc-r.github.io/discriminant_analysis
https://maths-people.anu.edu.au/~johnm/courses/mathdm/2008/pdf/r-exercisesVI.pdf
https://www.displayr.com/linear-discriminant-analysis-in-r-an-introduction/
https://tgmstat.wordpress.com/2014/01/15/computing-and-visualizing-lda-in-r/
https://rpubs.com/ifn1411/LDA
https://stackoverflow.com/questions/20197106/linear-discriminant-analysis-plot-using-ggplot2

brief outline of how we proceeded with the project:


