---
title: "Preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(reshape2)
library(caret)
library(MASS)
library(class)
library(gbm)
```

# (a) Data Split: Split the entire data (image1.txt, image2.txt, image3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

First, filter all unlabelled points as they can't be used to train the model. 
```{r}
image1 <- filter(image1,cloud_label != 0)
image2 <- filter(image2,cloud_label != 0)
image3 <- filter(image3,cloud_label != 0)
```


Two ways:
1) to just split the datasets into 80% train, 10% validation, and 10% test without any transformations.

```{r}
set.seed(1997)
#' Split1
#' @param images_list list of all labelled image dataframes
#' @return images a list of three dataframes: train, test, and split
split1 <- function(images_list, spec = c(train = .8, test = .1, val = .1)){
  images_comb <- data.frame()
  for(i in 1:length(images_list)){
    images_comb <- rbind(images_comb,images_list[[i]])
  }
  g <-  sample(cut(seq(nrow(images_comb)), nrow(images_comb)*cumsum(c(0,spec)),
                 labels = names(spec)))
  res <-  split(images_comb,g)
  return(res)
}


images <- split1(list(image1,image2,image3))
imagestrain <- images$train
imagesval <- images$val
imagestest <- images$test
```

2) take 3pixel X 3pixel blocks and creating 'super pixels' to transform the data into a new set. This will decrease the number of data points, but should help reconcile the dependence relationship between the points.

```{r}
#cloud_avg function takes in 9 values(vector) and returns -1, 0, 1 depending on avg
cloud_avg <- function(vector){
  avg <- mean(vector)
  if(avg<0){
    return(-1)}
  else if(avg>0){
    return(1)}
  else{
    return(0)}
}

#' Super Pixelize
#'
#' @param images list of dataframes with correct column names
#'
#' @return timages, a data frame in the global environment with 1/9th the values, averaged by 3x3 blocks,no x,y cols

super_pixelize <- function(images){
  newimages <-data.frame(matrix(ncol = 9, nrow = 0))
  for(k in 1:length(images)){
    image <- images[[k]]
    xs <- seq(from=min(image$x), to=max(image$x), by=3)
    ys <- seq(from=min(image$y), to=max(image$y), by=3)
    ktrans <-data.frame(matrix(ncol = 9, nrow = 0))
    for(i in xs){
      for(j in ys){
        pts <- image %>%filter((x==i | x==i+1 | x== i+2)&(y==j | y==j+1 | y== j+2))
        if (nrow(pts)>0){
          new_row <- c(cloud_avg(pts$cloud_label),mean(pts$NDAI),
                      mean(pts$SD),mean(pts$CORR),mean(pts$rad_DF),mean(pts$rad_CF),
                      mean(pts$rad_BF),mean(pts$rad_AF),mean(pts$rad_AN))
          ktrans <- rbind(ktrans, new_row)
         }
      }
    }
    images_columns <- c("cloud_label","NDAI","SD","CORR","rad_DF","rad_CF",
                    "rad_BF","rad_AF","rad_AN" )
    colnames(newimages) <- images_columns
    colnames(ktrans) <- images_columns 
    newimages <- rbind(newimages,ktrans)
  }
  return(newimages)
}

#' Split2
#' @param images_comb A list of all image dataframes
#' @param spec vector of sets to create in the split
#' @return transformed images a list of three dataframes: train, test, and split

split2 <- function(images_comb, spec = c(train = .8, test = .1, val = .1)){
  transformed <- super_pixelize(images_comb)
  transformed_list <- list(transformed[1:floor(0.5*nrow(transformed)),],
                           transformed[floor(0.5*nrow(transformed)):nrow(transformed),])
  return(split1(transformed_list, spec))
}

#timages$val

image_list <- list(image1,image2,image3)
timages <- split2(image_list)
timagestrain <- timages$train
timagesval <- timages$val
timagestest <- timages$test

```

# (b) Baseline: Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

```{r, warning=FALSE}
trivial <- rep(-1, nrow(imagesval))
ttrivial1 <- rep(-1, nrow(timagestest))
ttrivial2 <- rep(-1, nrow(timagesval))

data.frame(
  data_set = c("first_split_val", "first_split_test", "second_split_val", "second_split_test"),
  trivial_classifier_accuracy = c(mean(imagesval$cloud_label == trivial),
                                  mean(imagestest$cloud_label == trivial),
                                  mean(timagesval$cloud_label == ttrivial2),  
                                  mean(timagestest$cloud_label == ttrivial1)))
```

A trivial classifier has about a 60% accuracy on both the validation and test data sets. This kind of classifier will have high average accuracy if the actual images depict mostly cloudless regions. 

# (c) First order importance: Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

The farther the spread and least overlep between the two distributions-- cloud and no cloud-- the better. We expect the distribution of cloud vs no clouds to be different, so NDAI seems to be the 'best' feature for distinguishing between the two. Moreover, we want to pick a feature that can unambiguously distinguish between the two labels. This simplifies to a binary classification problem. So, similarly to a decision stump, we can use NDAI as a feature to train our classifier since it has minimal error - a small overlap of histograms. We will asses this error directly in ROC curves, where we compare true positive, and false positive rates. 
```{r}
#create cloud column of strings not factors, for histograms
imagestrain <- mutate(imagestrain, cloud = ifelse(cloud_label==1, "Cloud", "No Cloud"))
timagestrain <- mutate(timagestrain, cloud = ifelse(cloud_label==1, "Cloud", "No Cloud"))

#make histograms
ggplot(imagestrain,aes(x=NDAI, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + 
  ggtitle("Histogram of NDAI based on cloud label: first split method") 

ggplot(timagestrain,aes(x=NDAI, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + 
  ggtitle("Histogram of NDAI based on cloud label: second split method") 
```

```{r}
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud
ggplot(imagestrain,aes(x=CORR, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of CORR based on cloud label: first split")

ggplot(timagestrain,aes(x=CORR, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of CORR based on cloud label: second split")
```

```{r}
ggplot(imagestrain,aes(x=SD, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of SD based on cloud label: first split method")

ggplot(timagestrain,aes(x=SD, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of SD based on cloud label: second split method")

# SD seems to be a better feature on the super-pixelized data set... there's less overlap in the plots
```

```{r}
# run PCA on the angles to create a 'super feature', and see how it performs when predicting cloud label

pca_angles <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=imagestrain,
                     scale. = TRUE)

loadings <- pca_angles$rotation
scores <- pca_angles$x
eigenvalues <- pca_angles$sdev^2

eigs_cum = cumsum(eigenvalues) / sum(eigenvalues)

scree_plot <- ggplot() + 
  geom_point(aes(x = 1:length(eigenvalues), y=eigs_cum)) +
  labs(x = "Principal Component", y = "Fraction of Total Variance Explained") +
  ggtitle("Screeplot") + 
  theme_minimal()

scree_plot
PC1 <- scores[,1]

#So were not adding PC column to imagestrain? I have no preference 
train_pca <- imagestrain
train_pca$PC1 <- PC1

ggplot(train_pca, aes(x=PC1, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + ggtitle("Histogram of PC1 based on cloud label: first split method")

ggplot(train_pca, aes(x=cloud_label, y=PC1)) +
  geom_violin() +
  theme_minimal() + 
  ggtitle("Radiance angle PC1 and cloud label")

```
From the scree plot, PC1 captures 85% of the variance of all five radiance angles using the first split method. It does not do a good job of predicting cloud labels however, and therefore we will not use any of the radiance angles, nor the respective principal component as a feature to train our classifier. 

# (d) Write a generic cross validation (CV) function CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.
```{r} 
#Loss Functions- Are some of these built into R?
# write a function that runs exponential loss, which is what boosting takes...
#Example loss function it should be able to take in:
zero_one_loss <- function(predicted, expert){
  return(mean(predicted == expert))
}  

```

From piazza: Clarification about CVgeneric on April 22: 
"CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K, and a loss function":

- training features/labels: here the training features/labels mean all data points except those in the test set. So your CVgeneric will run the split and the classification algorithm inside the function

- loss function: any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
      L2 loss, because there's no need to induce sparsity, or exponential loss, etc. 

```{r}
#' CVgeneric
#'
#' @param classifier string name of function
#' add clauses
#' @param data list of image dataframes or combined image df, with all features
#' @param k number of folds
#' @param splitter function to split data
#' @param loss  any function that takes true labels and predicted label/predicted class
#' @param featstrain vector of string names of feature columns
#' @param labeltrain string name of label column
#'
#' @return k-fold cv loss on training set(split via second, better method)

CVgeneric <- function(classifier, data, K, loss, splitter=split1, featstrain, labeltrain){
  set.seed(12345678)
  split_data <- splitter(data, spec=c(train=0.9,test=0.1))
  datatrain <- split_data$train
  #still the error
  folds <- createFolds(datatrain$cloud_label, k = K)
  fold_loss <- rep(0,K)
  
  for (i in 1:K){
    val_dat <- datatrain[folds[[i]],]
    train_dat <- datatrain[-folds[[i]],]
    
    #knn
    if (classifier=="knn"){
      datclassed <- knn(train_dat[,4:6], val_dat[,4:6],train_dat$cloud_label,k=10)
    }
    
    #logistic
    else if (classifier=="glm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- glm(formula, data=train_dat, family="binomial")
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    
    #lda
    else if(classifier=="lda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- lda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #qda
    else if(classifier=="qda"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      model <- qda(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    }
    
    #boosting
    else if(classifier=="gbm"){
      formula <- as.formula(paste(labeltrain, "~",  paste(featstrain, collapse = "+")))
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1, 0)
      val_dat$cloud_label <- replace(val_dat$cloud_label,
                                       val_dat$cloud_label == -1, 0)
      model <- gbm(formula, data = train_dat, distribution = "gaussian")
      model.pred <- predict(model, val_dat, n.trees = 100)
      datclassed <- rep(0, length(model.pred))
      datclassed[model.pred >= 0.5] = 1
    }
    #This part is the problem of QDA
    fold_loss[i] <- loss(datclassed, val_dat$cloud_label) 
  }  
  return(fold_loss)
}
```

