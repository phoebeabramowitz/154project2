---
title: "Modeling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 3. We now try to fit different classification models and assess the fitted models using different criterion. For the next three parts, we expect you to try logistic regression and at least three other methods. 
  
##########I think we should do logistic, QDA, SVM, and KNN --- ive never coded SVM but it shouldnt be too bad. And I need to figure out how to run KNN with only a subset of features, and not the whole dataset, but I have an idea of how to do it. (assuming we're only supposed to use the 'best' three features from problem 2)

Here are some good links to use as reference(they also apply to problem 4):
http://uc-r.github.io/discriminant_analysis
https://maths-people.anu.edu.au/~johnm/courses/mathdm/2008/pdf/r-exercisesVI.pdf
https://www.displayr.com/linear-discriminant-analysis-in-r-an-introduction/
https://tgmstat.wordpress.com/2014/01/15/computing-and-visualizing-lda-in-r/
https://rpubs.com/ifn1411/LDA
https://stackoverflow.com/questions/20197106/linear-discriminant-analysis-plot-using-ggplot2


# (a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

```{r}
# logistic regression only works with values between 0 and 1, so I removed unlabled points from the first split sets, and then replaced -1 with 0.

log_imagestrain <- imagestrain %>% filter(cloud_label != 0)
log_imagesval <- imagesval %>% filter(cloud_label != 0)
log_imagestest <- imagestest %>% filter(cloud_label != 0)

log_imagestrain$cloud_label <- replace(log_imagestrain$cloud_label, 
                                       log_imagestrain$cloud_label == -1, 
                                       0)

log_imagestest$cloud_label <- replace(log_imagestest$cloud_label, 
                                       log_imagestest$cloud_label == -1, 
                                       0)

log_imagesval$cloud_label <- replace(log_imagesval$cloud_label, 
                                       log_imagesval$cloud_label == -1, 
                                       0)

# logistic regression on first split has 88% accuracy on the validation set


first_log <- glm(cloud_label ~ NDAI + CORR + SD, data=log_imagestrain, family = binomial)

first_log.pred <- predict(first_log, log_imagesval, type="response")
first_pred <- rep(0, length(first_log.pred))
first_pred[first_log.pred > 0.5] = 1

mean(first_pred == log_imagesval$cloud_label)

# how do i incorporate cross validation into this? and should i combine the test and val set like the problem says? right now the model trains on imagestrain, and compares to imagesval
```


# (b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.



# (c) (Bonus) Assess the fit using other relevant metrics.