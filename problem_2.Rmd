---
title: "Preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(reshape2)
library(caret)
library(MASS)
```

# (a) Data Split: Split the entire data (image1.txt, image2.txt, image3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

Two ways:
1) to just split the datasets into 80% train, 10% validation, and 10% test without any transformations.

```{r}
set.seed(1997)
#' Split1
#' @param images_comb dataframe of all labelled image datapoints(rbind)
#' @return images a list of three dataframes: train, test, and split
split1 <- function(images_comb, spec = c(train = .8, test = .1, val = .1)){
  images_comb <- data.frame(images_comb)
  images_comb <- filter(images_comb,cloud_label != 0)
  g <-  sample(cut(seq(nrow(images_comb)), nrow(images_comb)*cumsum(c(0,spec)),
                 labels = names(spec)))
  res <-  split(images_comb,g)
  return(res)
}

images <- split1(rbind(image1,image2,image3))
imagestrain <- images$train
imagesval <- images$val
imagestest <- images$test
```

2) take 3pixel X 3pixel blocks and creating 'super pixels' to transform the data into a new set. This will decrease the number of data points, but should help reconcile the dependence relationship between the points.

```{r}
##Ignore all of these notes, just for me
#establish which pixels are next to each other in 3x3 pixel blocks
#idea: Start in lower left corner and iterate(while or for loop?)  until all pixels are grouped
#image ranges: 
#y(all): 2-383 with no gaps(382)
###1 is x:65-369, with no gaps(305)   
###2 is x:65-368, no gaps(304) 
###3 is 65-369, no gaps(305)
#last case/outer region might be uneven, how to deal with that?
#rename the x and y consecutivley, not on average(so we can do mapping thing) 
#Average of three points for cloud_label: nonzero = cloud, <0= no cloud
```

```{r}
#cloud_avg function takes in 9 values(vector) and returns -1, 0, 1 depending on avg
cloud_avg <- function(vector){
  avg <- mean(vector)
  if(avg<0){
    return(-1)}
  else if(avg>0){
    return(1)}
  else{
    return(0)}
}

#' Super Pixelize
#'
#' @param images list of dataframes with correct column names
#'
#' @return timages, a data frame in the global environment with 1/9th the values, averaged by 3x3 blocks,no x,y cols
# super_pixelize <- function(images){
#   newimages <-data.frame(matrix(ncol = 9, nrow = 0))
#   for(k in 1:length(images)){
#     image <- images[[k]]
#     xs <- seq(from=min(image$x), to=max(image$x), by=3)
#     ys <- seq(from=min(image$y), to=max(image$y), by=3)
#     ktrans <-data.frame(matrix(ncol = 9, nrow = 0))
#     for(i in xs){
#      for(j in ys){
#         pts <- image %>%filter((x==i | x==i+1 | x== i+2)&(y==j | y==j+1 | y== j+2))
#         if (nrow(pts)>0){
#           new_row <- c(cloud_avg(pts$cloud_label),mean(pts$NDAI),
#                       mean(pts$SD),mean(pts$CORR),mean(pts$rad_DF),mean(pts$rad_CF),
#                       mean(pts$rad_BF),mean(pts$rad_AF),mean(pts$rad_AN))
#           ktrans <- rbind(ktrans, new_row)
#         }
#       }
#     }
#     newimages <- rbind(newimages, ktrans)
#   }  
#   images_columns <- c("cloud_label","NDAI","SD","CORR","rad_DF","rad_CF",
#                     "rad_BF","rad_AF","rad_AN" )
#   colnames(newimages) <- images_columns 
#   return(newimages)
# }

super_pixelize <- function(images){
  newimages <-data.frame(matrix(ncol = 9, nrow = 0))
  for(k in 1:length(images)){
    image <- images[[k]]
    #filter out unlabelled here
    image <- filter(image,cloud_label != 0)
    xs <- seq(from=min(image$x), to=max(image$x), by=3)
    ys <- seq(from=min(image$y), to=max(image$y), by=3)
    ktrans <-data.frame(matrix(ncol = 9, nrow = 0))
    for(i in xs){
      for(j in ys){
        pts <- image %>%filter((x==i | x==i+1 | x== i+2)&(y==j | y==j+1 | y== j+2))
        if (nrow(pts)>0){
          new_row <- c(cloud_avg(pts$cloud_label),mean(pts$NDAI),
                      mean(pts$SD),mean(pts$CORR),mean(pts$rad_DF),mean(pts$rad_CF),
                      mean(pts$rad_BF),mean(pts$rad_AF),mean(pts$rad_AN))
          ktrans <- rbind(ktrans, new_row)
         }
      }
    }
    images_columns <- c("cloud_label","NDAI","SD","CORR","rad_DF","rad_CF",
                    "rad_BF","rad_AF","rad_AN" )
    colnames(newimages) <- images_columns
    colnames(ktrans) <- images_columns 
    newimages <- rbind(newimages,ktrans)
  }
  return(newimages)
}

#' Split2
#' @param images_comb A list of all image dataframes
#' @param spec vector of sets to create in the split
#' @return transformed images a list of three dataframes: train, test, and split

split2 <- function(images_comb, spec = c(train = .8, test = .1, val = .1)){
  transformed <- super_pixelize(images_comb)
  return(split1(transformed, spec))
}

image_list <- list(image1,image2,image3)
timages <- split2(image_list)
timagestrain <- timages$train
timagesval <- timages$val
timagestest <- timages$test

```

# (b) Baseline: Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

```{r, warning=FALSE}
trivial <- rep(-1, nrow(imagesval))
ttrivial1 <- rep(-1, nrow(timagestest))
ttrivial2 <- rep(-1, nrow(timagesval))

data.frame(
  data_set = c("first_split_val", "first_split_test", "second_split_val", "second_split_test"),
  trivial_classifier_accuracy = c(mean(imagesval$cloud_label == trivial),
                                  mean(imagestest$cloud_label == trivial),
                                  mean(timagesval$cloud_label == ttrivial2),  
                                  mean(timagestest$cloud_label == ttrivial1)))
```

A trivial classifier has about a 61% accuracy on both the validation and test data sets. This kind of classifier will have high average accuracy if the actual images depict mostly cloudless regions. 

# (c) First order importance: Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

The farther the spread and least overlep between the two distributions-- cloud and no cloud-- the better. We expect the distribution of cloud vs no clouds to be different, so NDAI seems to be the 'best' feature for distinguishing between the two. This simplifies to a binary classification problem. So, similarly to a decision stump, we can use NDAI as a feature to train our classifier since it has minimal error (small overlap of histograms). We can asses this error directly in ROC curves, where we compare true positive, and false positive rates. 
```{r}
#create cloud column of strings not factors, for histograms
imagestrain <- mutate(imagestrain, cloud = ifelse(cloud_label==1, "Cloud", "No Cloud"))
timagestrain <- mutate(timagestrain, cloud = ifelse(cloud_label==1, "Cloud", "No Cloud"))

#make histograms
ggplot(imagestrain,aes(x=NDAI, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + 
  ggtitle("Histogram of NDAI based on cloud label: first split method") 

ggplot(timagestrain,aes(x=NDAI, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + 
  ggtitle("Histogram of NDAI based on cloud label: second split method") 
```

```{r}
# need to add legend... 1 (blue) is with cloud, -1 (green) is no cloud
ggplot(imagestrain,aes(x=CORR, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of CORR based on cloud label: first split")

ggplot(timagestrain,aes(x=CORR, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of CORR based on cloud label: second split")
```

```{r}
ggplot(imagestrain,aes(x=SD, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of SD based on cloud label: first split method")

ggplot(timagestrain,aes(x=SD, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() +
  ggtitle("Histogram of SD based on cloud label: second split method")

# SD seems to be a better feature on the super-pixelized data set... there's less overlap in the plots
```

```{r}
# run PCA on the angles to create a 'super feature', and see how it performs when predicting cloud label

pca_angles <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=imagestrain,
                     scale. = TRUE)

loadings <- pca_angles$rotation
scores <- pca_angles$x
eigenvalues <- pca_angles$sdev^2

eigs_cum = cumsum(eigenvalues) / sum(eigenvalues)

scree_plot <- ggplot() + 
  geom_point(aes(x = 1:length(eigenvalues), y=eigs_cum)) +
  labs(x = "Principal Component", y = "Fraction of Total Variance Explained") +
  ggtitle("Screeplot") + 
  theme_minimal()

scree_plot
PC1 <- scores[,1]

#So were not adding PC column to imagestrain? I have no preference 
train_pca <- imagestrain
train_pca$PC1 <- PC1

ggplot(train_pca, aes(x=PC1, fill=cloud)) + 
  geom_histogram(alpha=0.3, position="identity", bins=50) + 
  theme_classic() + ggtitle("Histogram of PC1 based on cloud label: first split method")

ggplot(train_pca, aes(x=cloud_label, y=PC1)) +
  geom_violin() +
  theme_minimal() + 
  ggtitle("Radiance angle PC1 and cloud label")

```
From the scree plot, PC1 captures 85% of the variance of all five radiance angles using the first split method. It does not do a good job of predicting cloud labels however, and therefore we will not use any of the radiance angles, nor the respective principal component as a feature to train our classifier. 
```{r}
# try again but this time with transformed data? (i dont think it'll make a difference)
# tpca_angles <- prcomp(~ rad_DF + rad_CF + rad_BF + rad_AF + rad_AN, data=timagestrain,
#                      scale. = TRUE)
# 
# tloadings <- tpca_angles$rotation
# tscores <- tpca_angles$x
# teigenvalues <- tpca_angles$sdev^2
# 
# teigs_cum = cumsum(eigenvalues) / sum(eigenvalues)
# 
# tscree_plot <- ggplot() + 
#   geom_point(aes(x = 1:length(teigenvalues), y=teigs_cum)) +
#   labs(x = "Principal Component", y = "Fraction of Total Variance Explained") +
#   ggtitle("Screeplot") + 
#   theme_minimal()
# 
# tscree_plot
# tPC1 <- scores[,1]
# 
# ttrain_pca <- timagestrain
# ttrain_pca$PC1 <- tPC1
# 
# ggplot(ttrain_pca, aes(x=tPC1, fill=cloud)) + 
#   geom_histogram(alpha=0.3, position="identity", bins=50) + 
#   theme_classic() + ggtitle("Histogram of PC1 based on cloud label: second split method") 
```
# (d) Write a generic cross validation (CV) function CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.
```{r}
#' CVgeneric
#'
#' @param classifier function?
#' add a clause for knn
#' @param data list of image dataframes or combined image df, with all features
#' @param k number of folds
#' @param splitter function to split data
#' @param loss  any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
#'
#' @return k-fold cv loss on training set(split via second, better method)

CVgeneric <- function(classifier, data, k, loss, splitter=split2){
  set.seed(12345678)
  split_data <- splitter(data, spec=c(train=0.9,test=0.1))
  datatrain <- split_data$train
  folds <- createFolds(datatrain$cloud_label, k = k)
  fold_loss <- rep(0,k)
  for(i in 1:k){
    val_dat <- datatrain[folds[[i]],]
    train_dat <- datatrain[-folds[[i]],]
    if(identical(classifier, "knn")){
      datclassed <- classifier(train_dat[,4:6], val_dat[,4:6],
                               train_dat$cloud_label, 10)
      # datclassed<- rep(0,10) 
      # for(j in 1:10){
      #   datclassed[j] <- classifier(train_dat[,4:6], val_dat[,4:6], train_dat$cloud_label)
      # }
    }  
    else if(identical(classifier, "glm")){
      formula <- as.formula(cloud_label ~ NDAI + CORR + SD)
      train_dat$cloud_label <- replace(train_dat$cloud_label,
                                       train_dat$cloud_label == -1,
                                       0)
      model <- classifier(formula, data=train_dat, family=binomial)
      model.pred <- predict(model, val_dat, type="response")
      datclassed <- rep(0, length(first_log.pred))
      datclassed[model.pred >= 0.5] = 1
    }  
    else{
      formula <- as.formula(cloud_label ~ NDAI + CORR + SD)
      model <- classifier(formula, train_dat)
      model.pred <- predict(model, val_dat)
      datclassed <- model.pred$class
    
    }
    ## This isn't working becuase arguments aren't of the same type. List? Double?
    fold_loss[i] <- loss(val_dat$cloud_label, datclassed) 
  }  
  accuracy <- mean(fold_loss)
  return(accuracy)
}

#this subset doesn't tell us anything, just if the code works
tiny_image_list <- list(image1[1:3000,],image2[1:3000,],image3[1:3000,])

# we need to change the cloud_label before running glm outside of CVgeneric function, doing it within the function doesn't work. 
#This isn't gonna work it's a list
# tiny_image_list$cloud_label <- replace(tiny_image_list$cloud_label,
#                                        tiny_image_list$cloud_label == -1,
#                                        0)

CVgeneric(classifier=lda, data=tiny_image_list, k=10, loss=zero_one_loss, splitter=split1)
    
#Example loss function it should be able to take in:
zero_one_loss <- function(predicted, expert){
  return(mean(predicted == expert))
}  

# Questions for office hours:
 # - createFolds is erroring for some reason when using glm
 # - knn is having a dimensions issue, but it works when I run it outside of CVgeneric
 # - we're getting weirdly high accuracies, and i'm not sure why that .. 
 #   (could it b/c of the subset of data we're testing this on?)
```
```{r}
# debugging...

test_split <- split1(tiny_image_list, spec=c(train=0.9, test=0.1))
test_split_train <- test_split$train
test_split_test <- test_split$test

class <- knn(test_split_train[,4:6], test_split_test[,4:6],
    test_split_train$cloud_label, 10)

mean(class == test_split_test$cloud_label)

# okay so im able to split1 to the tiny_image_list, and then run knn with k=10, so im not sure where the CVgeneric is erroring with "Error in dim(data) <- dim : invalid first argument"

# at the same time however, this knn is getting 99% accuracy, which is super high..
```

```{r}
# debugging continued..
# knn seems to work here, and the dimensions are fine. not sure why it's erroring in CVgeneric
# still getting suuuppperrr high accuracies tho, and I have no idea why?!?!?
test_folds <- createFolds(test_split_train$cloud_label, k = 10)
fold_loss <- rep(0,10)
for(i in 1:10){
    val_dat <- test_split_train[test_folds[[i]],]
    train_dat <- test_split_train[-test_folds[[i]],]

    class <- knn(train_dat[,4:6], val_dat[,4:6],
    train_dat$cloud_label, 10)
    
    fold_loss[i] <- zero_one_loss(val_dat$cloud_label, class)
}

accuracy <- mean(fold_loss)

```

```{r}
# im going to try the same thing as above but with split2
split <- split2(tiny_image_list, spec=c(train=0.9, test=0.1))
split_train <- split$train
split_test <- split$test

```

From piazza: Clarification about CVgeneric on April 22: 
"CVgeneric in R that takes a generic classifier, training features, training labels, number of folds K, and a loss function":

- training features/labels: here the training features/labels mean all data points except those in the test set. So your CVgeneric will run the split and the classification algorithm inside the function

- loss function: any function that takes true labels and predicted label/predicted class probability that outputs some performance metric for your classifier. 
      L2 loss, because there's no need to induce sparsity, or exponential loss, etc. 


